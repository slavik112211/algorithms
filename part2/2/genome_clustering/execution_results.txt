1) clustering_500.txt   -  497 clusters
2) clustering_1000.txt  -  989 clusters

3) clustering_10000.txt - 9116 clusters

4) clustering_big.txt - 6118 out of 200000 clusters
Approx. running time on a 4-core 64-bit machine on JRuby: 4 hours 30 mins.

Run on a 64bit 4-cores laptop, 4 threads of execution.
1. ruby 2.1.0p0 (2013-12-25 revision 44422) [x86_64-linux]
Time: 75.62 sec. 
Execution: 3 out of 4 processor cores are loaded upto 40%, but total processor load never above 25%.
Seems like MRI Ruby 2.1 loads only 1 core at a time (25% of total 4 cores = 100% 1 core), 
even though distributes the load between the cores.

2. jruby 1.7.15 (1.9.3p392) 2014-09-03 82b5cc3 on OpenJDK 64-Bit Server VM 1.7.0_65-b32 +jit [linux-amd64]
Time: 28.79 sec.
Execution: JRuby is really able to load all 4 processing cores with work.
Processor load goes upto 100%, then lowers to 75%, then to 50%, then to 25%.
This reflects how threads are finishing their work - threads workload is distributed unevenly.
Thread1 has the biggest chunk of work, thread4 - the smallest. We distribute the outer
loop (i-iterator) evenly to quarters between all threads. But the inner-loop (j-iterator) is much smaller for the last quarter, than for the first: j_lower_bound=i+1

==================================

https://class.coursera.org/algo2-003/forum/thread?thread_id=97#comment-441

The typical way is to check for neighbours from each vertex. There may be cleverer solutions, but in three runs through the course I've only coded or seen three broad strategies:

1. For each vertex, generate every possible neighbour of hamming distance <= 2 and check whether it is in the input.

2. Sort the vertices into a tree based on their bit values. For each vertex, walk the tree (eg. using depth-first), turning back whenever following a branch would result in a hamming distance of 3 or more, and check which vertex leaves are encountered.

3. Sort the vertices into buckets, so that the neighbours of a vertex in one bucket can only lie within a small set of other buckets. Search those buckets exhaustively to find each vertex's neighbours.

All of these approaches admit optimizations to avoid checking for the same edge twice, which I've glossed over in the summaries above.